# -*- coding: utf-8 -*-
"""Water_Potability_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ldaGMza67LQcSCaOs2W9kf1chu6ts8j7
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split,RandomizedSearchCV
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix,accuracy_score
import joblib
import warnings

warnings.filterwarnings('ignore')
pd.pandas.set_option('display.max_columns',None)

dataset = pd.read_excel('waterpotability.xlsx')

"""# Let us perform some EDA on our dataset"""

dataset.head(10)

dataset.describe()

"""Using describe function we can see that there are many missing values in some features."""

dataset.isnull().sum()       #    ph, Sulfate, Trihalomethanes are the feature which have missing values

len(dataset.columns)

"""As we can see almost all distributions are gaussian distributed"""

#Split our data in feature and target
features = dataset.drop('Potability',axis = 1)
target = dataset.Potability
num_feat = [col for col in features.columns if features[col].dtypes != 'O']
feat_nan = [col for col in features.columns if features[col].isnull().any()]
x_train,x_test,y_train,y_test = train_test_split(features,target,test_size = 0.2,shuffle = False)

def plot_dist(df):
    count = 0
    plt.figure(figsize = (8,25))
    for i in df.columns:
        count = count + 1
        plt.subplot(len(df.columns),1,count)
        sns.distplot(df[i])

plot_dist(features)

def plot_box(df):
    c = 0
    plt.figure(figsize = (8,25))
    for i in df.columns:
        c = c + 1
        plt.subplot(len(df.columns),1,c)
        sns.boxplot(df[i])

"""let us replace all the nan values with random sample imputation"""

def impute(df):
    
    for col in df.columns:
      sample = df[col].dropna().sample(df[col].isnull().sum())
      sample.index = df[df[col].isnull()].index
      df.loc[df[col].isnull(),col] = sample
    return df

def handle_outliers(df):
    for feat in df.columns:
        q1 = df[feat].sort_values().quantile(0.25)
        q3 = df[feat].sort_values().quantile(0.75)
        iqr = q3 - q1
        lower_whisker = q1 - 1.5*iqr
        upper_whisker = q3 + 1.5*iqr
        feature = df[feat]
        idx = list(feature[(feature > upper_whisker) | (feature < lower_whisker)].index)
        df.loc[idx,feat] = feature.median()
        idx.clear()
    return df

def select_features(x_train):
    col_corr = set()  # Set of all the names of correlated columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if abs(corr_matrix.iloc[i, j]) > 0.08: # we are interested in absolute coeff value
                colname = corr_matrix.columns[i]  # getting the name of column
                col_corr.add(colname)
                
    return x_train[col_corr]

impute_features = FunctionTransformer(impute)

outlier_handler = FunctionTransformer(handle_outliers)

best_features = FunctionTransformer(select_features)

prepare_data = Pipeline([('impute',impute_features),('outliers',outlier_handler)])

train = prepare_data.fit_transform(x_train)

x_test = prepare_data.transform(x_test)

"""# Random Forest Classifier"""

clf1 = RandomForestClassifier()
n_estimators = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)] # number of trees in the random forest
max_features = ['auto', 'sqrt'] # number of features in consideration at every split
max_depth = [int(x) for x in np.linspace(10, 120, num = 12)] # maximum number of levels allowed in each decision tree
min_samples_split = [2, 6, 10] # minimum sample number to split a node
min_samples_leaf = [1, 3, 4] # minimum sample number that can be stored in a leaf node
bootstrap = [True, False] # method used to sample data points
param_rfc = {'n_estimators':n_estimators,'max_features':max_features,'max_depth':max_depth,'min_samples_split':min_samples_split,'min_samples_leaf':min_samples_leaf,'bootstrap':bootstrap}
rfc = RandomizedSearchCV(clf1,param_rfc)
rfc.fit(x_train,y_train)
print(rfc.best_params_)         #best parameters are :- 
print(rfc.best_score_)          #best score is :-

pred_rfc = rfc.predict(x_test)
confusion_matrix(y_test,pred_rfc)

model = RandomForestClassifier(n_estimators= 16, min_samples_split= 6, min_samples_leaf= 4, max_features= 'sqrt', max_depth= 10, bootstrap= True)
model.fit(x_train,y_train)
pred = model.predict(x_test)
accuracy_score(y_test,pred)

import joblib
joblib.dump(model,'classifier.sav')